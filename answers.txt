Ответы на допвопросы

Проект участия - разработка междивизиональной (для двух предприятий) онлайн-платформы
* Роль - системный аналитик. Запрос требований у заказчиков-специалистов энероучёта, фиксация багов и формулирование тасков разработчикам в Azure DevOps
* Команда - 15 человек, включая двух РП по ИТ со стороны каждого предприятия
* Стек - ASP.NET + MS SQL Server
* Назначение - отображение производственных показателей в виде графиков, мнемосхем и таблиц
* Внедрённые интеграции со сторонними сервисами - изначально за авторизацию отвечал RabbitMQ, но он постоянно отваливался, перешли на другое решение (нет информации, какое)
* Решения, используемые в проекте - сервер приложения, расположенный на одном ппредприятии, из соображений безопасности не может опрашивать сервер данных другого предприятия, перекачку делаем через веб-интерфейс. Но всё же планируем попробовать прямой DBLink база-база.
* Покрытие тестами - нет информации, но по корпоративному стандарту почти наверняка есть
* ПО для микросервисной архитектуры - брокер RabbitMQ для авторизации, но от него, как я сказал, ушли. Разрабатываем свой драйвер для парсинга через веб. Да и вообще, микро там и не наблюдается
* Нестандартные решения - свой конфигуратор "тегов" (привязка id приборов к дереву узлов учёта), редактор мнемосхем с возможностью подгрузки заготовленного фона и размещения на нём транспарантов, в т.ч. интерактивных.
* Готовые решения - нет информации

Проект для рабочих задач - автоматизация сбора, обработки и выдачи показателей
* Роль - разработчик. Архитектура, набор данных и пр.
* Команда - 1
* Стек - Python+Flask
* Назначение - парсинг данных с веб-интерфейса, SQL-серверов, агрегирование (join, group), выдача в виде WebAPI
* Сторонние сервисы - нет (библиотеки же не считаются)
* Решения - веб-источник может отдавать только один показатель за один запрос, отдаю перечень тегов, по ним циклом прогоняются запросы, всё собирается в Pandas.Dataframe, с DF уже идёт работа по объединению, группировке (получасовые данные нужно собрать в часовые и иногда в суточные), результат записывается в промежуточные csv, либо весь аглоритм вызывается по GET-запросу и отдаётся сразу же пользователю. Также есть ещё не переведённая на Python автоматизация, она крутится на KNIME, который пишет csv, которые приложение подхватиывает ои отдаёт по запросу пользователя.
* Тесты - нет, но старюсь оборачивать в try
* Микросервис - нет, монолит
* Нестандартные решения - Чтобы уйти от csv или запускать KNIME на другой машине, запись в csv заменял на POST-запрос, приложение принимает запрос, если он POST, значит это KNIME и данные сохраняются, если GET - значит это пользователь (браузер или PowerBI) и сохранённое надо отдать.
* Готовые решения - нет
